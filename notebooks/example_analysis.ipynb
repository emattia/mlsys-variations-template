{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Data Analysis Notebook\n",
    "\n",
    "This notebook demonstrates how to structure a data analysis workflow using the project template. It includes examples of loading data, exploratory data analysis, feature engineering, model training, and evaluation.\n",
    "\n",
    "## NBDoc Documentation\n",
    "\n",
    "This notebook is documented using NBDoc, which allows for generating documentation from Jupyter notebooks. NBDoc uses special comment syntax to mark sections of the notebook for documentation generation.\n",
    "\n",
    "<!-- #nbdoc:title Example Data Analysis -->\n",
    "<!-- #nbdoc:description This notebook demonstrates a complete data analysis workflow using the project template. -->\n",
    "<!-- #nbdoc:version 0.1.0 -->\n",
    "<!-- #nbdoc:author Your Name -->\n",
    "<!-- #nbdoc:keywords data analysis, machine learning, visualization -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "<!-- #nbdoc:description Import libraries and configure the environment -->": "",
    "<!-- #nbdoc:section Setup -->": ""
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data processing\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Add the project root to the path so we can import our modules.\n",
    "# A bit hacky, but we use because it is commonly used, easy to understand, and it works.\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "from src.tabular_data_utils import (\n",
    "    calculate_feature_importance,\n",
    "    encode_categorical,\n",
    "    normalize_features,\n",
    "    split_train_test,\n",
    ")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logging.getLogger(\"matplotlib.category\").setLevel(logging.WARNING)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set plot style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Let's load a sample dataset for our analysis. In a real project, you would replace this with your actual data loading code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "<!-- #nbdoc:description Load and prepare the dataset for analysis -->": "",
    "<!-- #nbdoc:section Data Loading -->": ""
   },
   "outputs": [],
   "source": [
    "# For demonstration, we'll create a synthetic dataset\n",
    "# In a real project, you would use load_dataset() to load your data\n",
    "\n",
    "# Create synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Numeric features\n",
    "feature1 = np.random.normal(0, 1, n_samples)\n",
    "feature2 = np.random.normal(5, 2, n_samples)\n",
    "feature3 = np.random.uniform(0, 10, n_samples)\n",
    "\n",
    "# Categorical features\n",
    "categories = [\"A\", \"B\", \"C\"]\n",
    "category1 = np.random.choice(categories, n_samples)\n",
    "category2 = np.random.choice([\"X\", \"Y\"], n_samples)\n",
    "\n",
    "# Target variable (binary classification)\n",
    "# Target depends on features to create a pattern\n",
    "target_probs = 1 / (\n",
    "    1\n",
    "    + np.exp(\n",
    "        -(\n",
    "            0.5 * feature1\n",
    "            - 0.2 * feature2\n",
    "            + 0.1 * feature3\n",
    "            + 0.5 * (category1 == \"A\")\n",
    "            + 0.7 * (category2 == \"X\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "target = np.random.binomial(1, target_probs)\n",
    "\n",
    "# Create DataFrame\n",
    "data = {\n",
    "    \"feature1\": feature1,\n",
    "    \"feature2\": feature2,\n",
    "    \"feature3\": feature3,\n",
    "    \"category1\": category1,\n",
    "    \"category2\": category2,\n",
    "    \"target\": target,\n",
    "}\n",
    "\n",
    "df = pl.DataFrame(data)\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Let's explore the dataset to understand its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "<!-- #nbdoc:description Analyze the dataset to understand its characteristics -->": "",
    "<!-- #nbdoc:section Exploratory Data Analysis -->": ""
   },
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nNumeric columns statistics:\")\n",
    "df.select(pl.col([\"feature1\", \"feature2\", \"feature3\"])).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of categorical variables\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "category1_counts = df.group_by(\"category1\").agg(pl.len()).sort(\"len\", descending=True)\n",
    "category2_counts = df.group_by(\"category2\").agg(pl.len()).sort(\"len\", descending=True)\n",
    "\n",
    "sns.barplot(\n",
    "    x=category1_counts[\"category1\"].to_numpy(),\n",
    "    y=category1_counts[\"len\"].to_numpy(),\n",
    "    ax=axes[0],\n",
    ")\n",
    "axes[0].set_title(\"Distribution of Category1\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "sns.barplot(\n",
    "    x=category2_counts[\"category2\"].to_numpy(),\n",
    "    y=category2_counts[\"len\"].to_numpy(),\n",
    "    ax=axes[1],\n",
    ")\n",
    "axes[1].set_title(\"Distribution of Category2\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of numeric features\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Feature1 distribution\n",
    "sns.histplot(df[\"feature1\"].to_numpy(), kde=True, ax=axes[0])\n",
    "axes[0].set_title(\"Distribution of Feature1\")\n",
    "\n",
    "# Feature2 distribution\n",
    "sns.histplot(df[\"feature2\"].to_numpy(), kde=True, ax=axes[1])\n",
    "axes[1].set_title(\"Distribution of Feature2\")\n",
    "\n",
    "# Feature3 distribution\n",
    "sns.histplot(df[\"feature3\"].to_numpy(), kde=True, ax=axes[2])\n",
    "axes[2].set_title(\"Distribution of Feature3\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distributionplt.figure(figsize=(10, 6))\n",
    "\n",
    "target_counts = df.group_by(\"target\").agg(pl.len())\n",
    "sns.barplot(x=target_counts[\"target\"].to_numpy(), y=target_counts[\"len\"].to_numpy())\n",
    "plt.title(\"Distribution of Target Variable\")\n",
    "plt.xlabel(\"Target\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks([0, 1], [\"Class 0\", \"Class 1\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Polars DataFrame to pandas for seaborn\n",
    "df_pandas = df.to_pandas()\n",
    "\n",
    "# Relationship between features and target\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Feature1 vs Target\n",
    "sns.boxplot(x=\"target\", y=\"feature1\", data=df_pandas, ax=axes[0])\n",
    "axes[0].set_title(\"Feature1 vs Target\")\n",
    "axes[0].set_xlabel(\"Target\")\n",
    "axes[0].set_ylabel(\"Feature1\")\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_xticklabels([\"Class 0\", \"Class 1\"])\n",
    "\n",
    "# Feature2 vs Target\n",
    "sns.boxplot(x=\"target\", y=\"feature2\", data=df_pandas, ax=axes[1])\n",
    "axes[1].set_title(\"Feature2 vs Target\")\n",
    "axes[1].set_xlabel(\"Target\")\n",
    "axes[1].set_ylabel(\"Feature2\")\n",
    "axes[1].set_xticks([0, 1])\n",
    "axes[1].set_xticklabels([\"Class 0\", \"Class 1\"])\n",
    "\n",
    "# Feature3 vs Target\n",
    "sns.boxplot(x=\"target\", y=\"feature3\", data=df_pandas, ax=axes[2])\n",
    "axes[2].set_title(\"Feature3 vs Target\")\n",
    "axes[2].set_xlabel(\"Target\")\n",
    "axes[2].set_ylabel(\"Feature3\")\n",
    "axes[2].set_xticks([0, 1])\n",
    "axes[2].set_xticklabels([\"Class 0\", \"Class 1\"])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Now let's prepare the data for modeling by normalizing numeric features and encoding categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "<!-- #nbdoc:description Prepare the data for modeling by transforming features -->": "",
    "<!-- #nbdoc:section Feature Engineering -->": ""
   },
   "outputs": [],
   "source": [
    "# Normalize numeric features\n",
    "numeric_columns = [\"feature1\", \"feature2\", \"feature3\"]\n",
    "df_normalized = normalize_features(df, columns=numeric_columns, method=\"standard\")\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_columns = [\"category1\", \"category2\"]\n",
    "df_processed = encode_categorical(\n",
    "    df_normalized, columns=categorical_columns, method=\"one_hot\"\n",
    ")\n",
    "\n",
    "# Display the processed data\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Let's split the data into training and testing sets, and train a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "<!-- #nbdoc:description Train a machine learning model on the prepared data -->": "",
    "<!-- #nbdoc:section Model Training -->": ""
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = split_train_test(\n",
    "    df_processed, target_column=\"target\", test_size=0.2, random_seed=42\n",
    ")\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "\n",
    "print(\"Model trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Let's evaluate the model's performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "<!-- #nbdoc:description Evaluate the model's performance on the test set -->": "",
    "<!-- #nbdoc:section Model Evaluation -->": ""
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test.to_numpy())\n",
    "\n",
    "# Calculate and display classification metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test.to_numpy(), y_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test.to_numpy(), y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xticks([0.5, 1.5], [\"Class 0\", \"Class 1\"])\n",
    "plt.yticks([0.5, 1.5], [\"Class 0\", \"Class 1\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and visualize feature importance\n",
    "feature_names = X_train.columns\n",
    "importance_values = model.feature_importances_\n",
    "\n",
    "# Get the top 10 most important features\n",
    "feature_importance = calculate_feature_importance(\n",
    "    feature_names, importance_values, top_n=10\n",
    ")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=list(feature_importance.values()), y=list(feature_importance.keys()))\n",
    "plt.title(\"Top 10 Feature Importance\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated a complete data workflow using the project template. We've covered:\n",
    "\n",
    "1. Data loading and exploration\n",
    "2. Feature engineering\n",
    "3. Model training\n",
    "4. Model evaluation\n",
    "\n",
    "This notebook serves as a template for your own data analysis projects. You can adapt it to your specific needs by replacing the synthetic data with your actual data and customizing the analysis steps as needed.\n",
    "\n",
    "<!-- #nbdoc:section Conclusion -->\n",
    "<!-- #nbdoc:description Summary of the analysis and next steps -->"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
