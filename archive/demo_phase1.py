#!/usr/bin/env python3
"""
Phase 1 Implementation Demonstration Script

This script demonstrates the core improvements implemented in Phase 1:
1. Unified Configuration Management with Pydantic + Hydra
2. Plugin Architecture with Abstract Base Classes
3. Enhanced Testing Infrastructure
4. Type-safe Configuration Validation

Run with: python demo_phase1.py
"""

import logging
import tempfile
from pathlib import Path

import numpy as np
import polars as pl

from src.config import AppConfig, ConfigManager, load_config
from src.plugins import (
    ExecutionContext,
    get_plugin,
    list_plugins,
)
from src.utils.common import create_run_id, setup_logging

# Import workflows to register plugins
try:
    import workflows.model_training  # This registers the sklearn_trainer plugin
except ImportError:
    pass  # Skip if dependencies are missing

# Configure logging for demo
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def demo_configuration_system():
    """Demonstrate the unified configuration system."""
    print("\n" + "=" * 60)
    print("ğŸ”§ CONFIGURATION SYSTEM DEMONSTRATION")
    print("=" * 60)

    print("\n1. Creating default configuration...")
    config = AppConfig()
    print(f"   âœ… App Name: {config.app_name}")
    print(f"   âœ… Environment: {config.environment}")
    print(f"   âœ… Random Seed: {config.ml.random_seed}")
    print(f"   âœ… Model Type: {config.model.model_type}")

    print("\n2. Configuration validation in action...")
    try:
        # Valid configuration
        valid_config = AppConfig(
            environment="production", ml={"test_size": 0.3, "cv_folds": 10}
        )
        print(
            f"   âœ… Valid config: Environment={valid_config.environment}, Test Size={valid_config.ml.test_size}"
        )
    except Exception as e:
        print(f"   âŒ Validation failed: {e}")

    try:
        # Invalid configuration
        invalid_config = AppConfig(
            environment="invalid_env",  # This should fail validation
        )
        print(f"   âŒ This shouldn't print: {invalid_config.environment}")
    except Exception as e:
        print(f"   âœ… Validation correctly caught invalid environment: {e}")

    print("\n3. Configuration overrides...")
    overrides = {
        "app_name": "demo-app",
        "ml": {"random_seed": 999, "hyperparameter_search": True},
        "model": {"model_type": "linear", "problem_type": "regression"},
    }

    config_with_overrides = load_config(overrides=overrides)
    print(f"   âœ… Overridden app name: {config_with_overrides.app_name}")
    print(f"   âœ… Overridden random seed: {config_with_overrides.ml.random_seed}")
    print(f"   âœ… Overridden model type: {config_with_overrides.model.model_type}")

    print("\n4. ConfigManager with Hydra integration...")
    with tempfile.TemporaryDirectory() as tmpdir:
        config_manager = ConfigManager(config_dir=Path(tmpdir) / "conf")
        config_manager.create_default_config_files()

        # Load config using Hydra
        hydra_config = config_manager.load_config()
        print(f"   âœ… Hydra config loaded: {hydra_config.app_name}")
        print(
            f"   âœ… Hydra-managed paths exist: {hydra_config.paths.data_raw.exists()}"
        )


def demo_plugin_architecture():
    """Demonstrate the plugin architecture."""
    print("\n" + "=" * 60)
    print("ğŸ”Œ PLUGIN ARCHITECTURE DEMONSTRATION")
    print("=" * 60)

    print("\n1. Listing available plugins...")
    plugins = list_plugins()
    print(f"   âœ… Found {len(plugins)} registered plugins: {plugins}")

    print("\n2. Getting plugin information...")
    plugins_with_info = list_plugins(with_info=True)
    for name, info in plugins_with_info.items():
        print(f"   ğŸ“¦ {name}: {info['description']} (v{info['version']})")
        print(f"      Category: {info['category']}")
        if info["dependencies"]:
            print(f"      Dependencies: {info['dependencies']}")

    print("\n3. Creating execution context...")
    config = AppConfig(app_name="demo-plugin-test")
    run_id = create_run_id("demo")

    context = ExecutionContext(
        config=config,
        run_id=run_id,
        component_name="demo_component",
        input_data={"demo": True},
    )

    print(f"   âœ… Run ID: {context.run_id}")
    print(f"   âœ… Artifacts dir: {context.artifacts_dir}")
    print("   âœ… Context created successfully")

    print("\n4. Plugin execution example...")
    if "sklearn_trainer" in plugins:
        try:
            # Create some sample data
            np.random.seed(42)
            n_samples = 200

            data = {
                "feature_1": np.random.randn(n_samples),
                "feature_2": np.random.randn(n_samples),
                "feature_3": np.random.uniform(0, 1, n_samples),
                "target": np.random.choice([0, 1], n_samples),
            }

            df = pl.DataFrame(data)
            print(f"   âœ… Created sample dataset: {df.shape}")

            # Update context with data
            context.input_data = {"dataframe": df}

            # Get and execute plugin
            trainer = get_plugin("sklearn_trainer")
            trainer.initialize(context)
            print(f"   âœ… Initialized trainer: {trainer.name}")

            result = trainer.execute(context)
            print(f"   âœ… Training execution: {result.status.value}")

            if result.is_success():
                print(
                    f"      ğŸ“Š CV Score: {result.metrics.get('cv_score_mean', 0):.4f}"
                )
                print(f"      â±ï¸  Execution time: {result.execution_time:.2f}s")
                print(
                    f"      ğŸ’¾ Model saved to: {result.artifacts.get('model', 'N/A')}"
                )
            else:
                print(f"      âŒ Training failed: {result.error_message}")

        except Exception as e:
            print(f"   âš ï¸  Plugin execution demo failed: {e}")
            print("      (This is expected if sklearn dependencies are missing)")
    else:
        print("   âš ï¸  sklearn_trainer plugin not found (normal during development)")


def demo_type_safety():
    """Demonstrate type safety and validation."""
    print("\n" + "=" * 60)
    print("ğŸ›¡ï¸  TYPE SAFETY & VALIDATION DEMONSTRATION")
    print("=" * 60)

    print("\n1. Pydantic model validation...")

    # Test path validation
    try:
        from src.config.models import PathsConfig

        paths = PathsConfig(data_raw="/tmp/test")
        print(f"   âœ… Path validation: {paths.data_raw}")
    except Exception as e:
        print(f"   âŒ Path validation failed: {e}")

    # Test constraint validation
    try:
        from src.config.models import MLConfig

        ml_valid = MLConfig(test_size=0.3, cv_folds=5)
        print(
            f"   âœ… Constraint validation: test_size={ml_valid.test_size}, cv_folds={ml_valid.cv_folds}"
        )
    except Exception as e:
        print(f"   âŒ Constraint validation failed: {e}")

    # Test invalid constraints
    try:
        from src.config.models import MLConfig

        ml_invalid = MLConfig(test_size=1.5)  # Should fail: > 1.0
        print(f"   âŒ This shouldn't print: {ml_invalid.test_size}")
    except Exception as e:
        print(f"   âœ… Constraint validation correctly caught invalid test_size: {e}")

    print("\n2. Enum validation...")
    try:
        from src.plugins.base import ComponentStatus

        status = ComponentStatus.SUCCESS
        print(f"   âœ… Enum usage: {status.value}")

        # Status checking methods
        from src.plugins.base import ComponentResult

        result = ComponentResult(
            status=ComponentStatus.SUCCESS, component_name="test", execution_time=1.0
        )
        print(
            f"   âœ… Status checking: is_success()={result.is_success()}, is_failed()={result.is_failed()}"
        )

    except Exception as e:
        print(f"   âŒ Enum validation failed: {e}")


def demo_enhanced_testing():
    """Demonstrate enhanced testing capabilities."""
    print("\n" + "=" * 60)
    print("ğŸ§ª ENHANCED TESTING DEMONSTRATION")
    print("=" * 60)

    print("\n1. Test fixtures and utilities...")

    # Demonstrate what the test fixtures provide
    with tempfile.TemporaryDirectory() as tmpdir:
        # Simulate test config creation
        test_config = AppConfig(
            app_name="test-app",
            environment="testing",
            paths={
                "data_raw": Path(tmpdir) / "data" / "raw",
                "models_trained": Path(tmpdir) / "models" / "trained",
                "logs_dir": Path(tmpdir) / "logs",
            },
        )

        test_config.create_directories()
        print(f"   âœ… Test directories created in: {tmpdir}")
        print(f"   âœ… Data directory exists: {test_config.paths.data_raw.exists()}")
        print(
            f"   âœ… Models directory exists: {test_config.paths.models_trained.exists()}"
        )

    print("\n2. Sample data generation...")
    # Simulate test data creation
    np.random.seed(42)
    n_samples = 100

    sample_data = {
        "feature_1": np.random.randn(n_samples),
        "feature_2": np.random.randn(n_samples),
        "target": np.random.choice([0, 1], n_samples),
    }

    df = pl.DataFrame(sample_data)
    print(f"   âœ… Sample classification data: {df.shape}")
    print(f"   âœ… Features: {[col for col in df.columns if col != 'target']}")
    print(f"   âœ… Target distribution: {df['target'].value_counts()}")

    print("\n3. Test execution context...")
    context = ExecutionContext(
        config=test_config,
        run_id="test-run-001",
        component_name="test-component",
        metadata={"test_mode": True},
    )

    print(f"   âœ… Test context created: {context.run_id}")
    print(f"   âœ… Test metadata: {context.metadata}")

    print("\n4. Plugin registry isolation...")
    from src.plugins.registry import get_registry

    registry = get_registry()
    initial_count = len(registry._plugins)

    # This would be done in test setup/teardown
    registry.clear()
    cleared_count = len(registry._plugins)

    print(
        f"   âœ… Registry isolation: {initial_count} plugins -> {cleared_count} plugins (cleared)"
    )


def demo_integration():
    """Demonstrate how all components work together."""
    print("\n" + "=" * 60)
    print("ğŸ¯ INTEGRATION DEMONSTRATION")
    print("=" * 60)

    print("\n1. End-to-end workflow simulation...")

    # Step 1: Load configuration with overrides
    config_overrides = {
        "app_name": "integration-demo",
        "ml": {
            "random_seed": 42,
            "test_size": 0.2,
            "cv_folds": 3,  # Faster for demo
        },
        "model": {
            "model_type": "random_forest",
            "problem_type": "classification",
            "target_column": "target",
        },
    }

    config = load_config(overrides=config_overrides)
    setup_logging(config)

    print(f"   âœ… Configuration loaded: {config.app_name}")

    # Step 2: Create execution context
    run_id = create_run_id("integration")
    context = ExecutionContext(
        config=config,
        run_id=run_id,
        component_name="integration_demo",
        metadata={"demo_type": "integration"},
    )

    print(f"   âœ… Execution context: {context.run_id}")

    # Step 3: Generate sample data
    np.random.seed(config.ml.random_seed)
    n_samples = 300

    # Create realistic classification data
    feature_1 = np.random.randn(n_samples)
    feature_2 = np.random.randn(n_samples)
    feature_3 = feature_1 * 0.5 + np.random.randn(n_samples) * 0.3

    # Create target with some signal
    target_prob = 1 / (1 + np.exp(-(feature_1 + feature_2 * 0.8 + feature_3 * 0.3)))
    target = np.random.binomial(1, target_prob, n_samples)

    data = {
        "feature_1": feature_1,
        "feature_2": feature_2,
        "feature_3": feature_3,
        "noise_feature": np.random.randn(n_samples),  # Noise feature
        "target": target,
    }

    df = pl.DataFrame(data)
    print(f"   âœ… Generated realistic dataset: {df.shape}")
    print(f"   âœ… Target balance: {df['target'].sum()}/{len(df)} positive samples")

    # Step 4: Execute plugin-based workflow
    if "sklearn_trainer" in list_plugins():
        try:
            context.input_data = {"dataframe": df}

            trainer = get_plugin("sklearn_trainer")
            trainer.initialize(context)

            result = trainer.execute(context)

            if result.is_success():
                print("   âœ… Model training successful!")
                print(
                    f"      ğŸ“Š CV Score: {result.metrics['cv_score_mean']:.4f} Â± {result.metrics['cv_score_std']:.4f}"
                )
                print(
                    f"      ğŸš€ Training samples: {result.output_data['train_samples']}"
                )
                print(f"      ğŸ¯ Test samples: {result.output_data['test_samples']}")
                print(f"      ğŸ’¾ Model saved: {result.artifacts['model'].name}")
                print(f"      â±ï¸  Total time: {result.execution_time:.2f}s")

                # Demonstrate metadata tracking
                print("   âœ… Metadata tracking:")
                print(f"      ğŸ”§ Model type: {result.metadata['model_type']}")
                print(f"      ğŸ² Random seed: {result.metadata['random_seed']}")
                print(f"      ğŸ“Š CV folds: {result.metadata['cv_folds']}")

            else:
                print(f"   âŒ Model training failed: {result.error_message}")

        except Exception as e:
            print(f"   âš ï¸  Integration demo failed: {e}")
    else:
        print("   âš ï¸  Sklearn trainer not available for integration demo")

    print("\n   âœ… Integration demo complete!")


def main():
    """Run all demonstrations."""
    print("ğŸš€ MLOps Template Phase 1 Implementation Demo")
    print("=" * 60)
    print("This demonstration showcases the improvements implemented in Phase 1:")
    print("â€¢ Unified Configuration Management (Pydantic + Hydra)")
    print("â€¢ Plugin Architecture with Abstract Base Classes")
    print("â€¢ Enhanced Testing Infrastructure")
    print("â€¢ Type Safety and Validation")

    try:
        demo_configuration_system()
        demo_plugin_architecture()
        demo_type_safety()
        demo_enhanced_testing()
        demo_integration()

        print("\n" + "=" * 60)
        print("ğŸ‰ PHASE 1 DEMONSTRATION COMPLETE!")
        print("=" * 60)
        print("\nKey achievements:")
        print("âœ… Configuration system consolidated and type-safe")
        print("âœ… Plugin architecture provides extensibility")
        print("âœ… Enhanced testing infrastructure ready")
        print("âœ… All components integrate seamlessly")
        print("\nNext steps (Phase 2):")
        print("ğŸ“‹ Experiment tracking integration (MLflow)")
        print("ğŸš€ Model serving with FastAPI")
        print("ğŸ³ Containerization and deployment")
        print("ğŸ”„ CI/CD pipeline enhancements")

    except Exception as e:
        print(f"\nâŒ Demo failed with error: {e}")
        import traceback

        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
