"""Advanced caching system for ML operations and API responses.

This module provides intelligent caching for:
- LLM API responses (OpenAI, etc.)
- Vector embeddings
- Model predictions
- Data processing results
- Cost optimization through deduplication
"""

import hashlib
import json
import pickle
import sqlite3
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Union
from dataclasses import dataclass, asdict
from contextlib import contextmanager
import logging

logger = logging.getLogger(__name__)


@dataclass
class CacheEntry:
    """Cache entry with metadata."""
    key: str
    value: Any
    created_at: float
    accessed_at: float
    access_count: int
    cost_saved: float
    ttl: Optional[float] = None
    tags: List[str] = None

    def __post_init__(self):
        if self.tags is None:
            self.tags = []


class CacheManager:
    """Multi-level cache manager with cost tracking."""

    def __init__(self, cache_dir: Union[str, Path] = "data/cache"):
        """Initialize cache manager.

        Args:
            cache_dir: Directory for cache storage
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # Initialize SQLite database for metadata
        self.db_path = self.cache_dir / "cache_metadata.db"
        self._init_database()

        # In-memory cache for frequently accessed items
        self.memory_cache: Dict[str, CacheEntry] = {}
        self.max_memory_items = 1000

        # Cache statistics
        self.stats = {
            "hits": 0,
            "misses": 0,
            "total_cost_saved": 0.0,
            "storage_size": 0
        }

    def _init_database(self):
        """Initialize SQLite database for cache metadata."""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS cache_entries (
                    key TEXT PRIMARY KEY,
                    created_at REAL,
                    accessed_at REAL,
                    access_count INTEGER,
                    cost_saved REAL,
                    ttl REAL,
                    tags TEXT,
                    file_path TEXT
                )
            """)

            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_accessed_at ON cache_entries(accessed_at)
            """)

            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_tags ON cache_entries(tags)
            """)

    def _generate_key(self, data: Any, prefix: str = "") -> str:
        """Generate cache key from data.

        Args:
            data: Data to generate key for
            prefix: Optional prefix for the key

        Returns:
            Cache key string
        """
        if isinstance(data, dict):
            # Sort dict for consistent hashing
            data_str = json.dumps(data, sort_keys=True)
        elif isinstance(data, (list, tuple)):
            data_str = json.dumps(data)
        else:
            data_str = str(data)

        hash_obj = hashlib.sha256(data_str.encode())
        key = hash_obj.hexdigest()[:16]  # Use first 16 chars

        return f"{prefix}_{key}" if prefix else key

    def get(self, key: str) -> Optional[Any]:
        """Get value from cache.

        Args:
            key: Cache key

        Returns:
            Cached value or None if not found/expired
        """
        # Check memory cache first
        if key in self.memory_cache:
            entry = self.memory_cache[key]
            if not self._is_expired(entry):
                entry.accessed_at = time.time()
                entry.access_count += 1
                self.stats["hits"] += 1
                return entry.value
            else:
                # Remove expired entry
                del self.memory_cache[key]

        # Check disk cache
        entry = self._get_from_disk(key)
        if entry and not self._is_expired(entry):
            entry.accessed_at = time.time()
            entry.access_count += 1

            # Add to memory cache if frequently accessed
            if entry.access_count > 2:
                self._add_to_memory_cache(key, entry)

            self._update_metadata(entry)
            self.stats["hits"] += 1
            return entry.value

        self.stats["misses"] += 1
        return None

    def set(self, key: str, value: Any, ttl: Optional[float] = None,
            cost_saved: float = 0.0, tags: List[str] = None) -> None:
        """Set value in cache.

        Args:
            key: Cache key
            value: Value to cache
            ttl: Time to live in seconds
            cost_saved: Cost saved by caching this item
            tags: Tags for categorization
        """
        now = time.time()
        entry = CacheEntry(
            key=key,
            value=value,
            created_at=now,
            accessed_at=now,
            access_count=1,
            cost_saved=cost_saved,
            ttl=ttl,
            tags=tags or []
        )

        # Save to disk
        self._save_to_disk(entry)

        # Add to memory cache
        self._add_to_memory_cache(key, entry)

        # Update statistics
        self.stats["total_cost_saved"] += cost_saved

        logger.debug(f"Cached item with key: {key}, cost saved: ${cost_saved:.4f}")

    def _is_expired(self, entry: CacheEntry) -> bool:
        """Check if cache entry is expired."""
        if entry.ttl is None:
            return False
        return time.time() - entry.created_at > entry.ttl

    def _get_from_disk(self, key: str) -> Optional[CacheEntry]:
        """Get cache entry from disk."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute(
                "SELECT * FROM cache_entries WHERE key = ?", (key,)
            )
            row = cursor.fetchone()

            if not row:
                return None

            # Load value from file
            file_path = Path(row[7])  # file_path column
            if not file_path.exists():
                # Clean up orphaned metadata
                conn.execute("DELETE FROM cache_entries WHERE key = ?", (key,))
                return None

            try:
                with open(file_path, 'rb') as f:
                    value = pickle.load(f)

                return CacheEntry(
                    key=row[0],
                    value=value,
                    created_at=row[1],
                    accessed_at=row[2],
                    access_count=row[3],
                    cost_saved=row[4],
                    ttl=row[5] if row[5] else None,
                    tags=json.loads(row[6]) if row[6] else []
                )
            except Exception as e:
                logger.error(f"Error loading cached value for key {key}: {e}")
                return None

    def _save_to_disk(self, entry: CacheEntry) -> None:
        """Save cache entry to disk."""
        # Save value to file
        file_path = self.cache_dir / f"{entry.key}.pkl"
        with open(file_path, 'wb') as f:
            pickle.dump(entry.value, f)

        # Update metadata in database
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT OR REPLACE INTO cache_entries
                (key, created_at, accessed_at, access_count, cost_saved, ttl, tags, file_path)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                entry.key,
                entry.created_at,
                entry.accessed_at,
                entry.access_count,
                entry.cost_saved,
                entry.ttl,
                json.dumps(entry.tags),
                str(file_path)
            ))

    def _update_metadata(self, entry: CacheEntry) -> None:
        """Update cache entry metadata."""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                UPDATE cache_entries
                SET accessed_at = ?, access_count = ?
                WHERE key = ?
            """, (entry.accessed_at, entry.access_count, entry.key))

    def _add_to_memory_cache(self, key: str, entry: CacheEntry) -> None:
        """Add entry to memory cache with LRU eviction."""
        if len(self.memory_cache) >= self.max_memory_items:
            # Remove least recently accessed item
            lru_key = min(
                self.memory_cache.keys(),
                key=lambda k: self.memory_cache[k].accessed_at
            )
            del self.memory_cache[lru_key]

        self.memory_cache[key] = entry

    def cache_llm_response(self, prompt: str, model: str, response: str,
                          cost: float = 0.0, ttl: float = 86400) -> str:
        """Cache LLM response with automatic key generation.

        Args:
            prompt: Input prompt
            model: Model name
            response: LLM response
            cost: Cost of the API call
            ttl: Cache TTL in seconds (default 24 hours)

        Returns:
            Cache key for the response
        """
        cache_data = {"prompt": prompt, "model": model}
        key = self._generate_key(cache_data, "llm")

        self.set(
            key=key,
            value=response,
            ttl=ttl,
            cost_saved=cost,
            tags=["llm", model]
        )

        return key

    def get_llm_response(self, prompt: str, model: str) -> Optional[str]:
        """Get cached LLM response.

        Args:
            prompt: Input prompt
            model: Model name

        Returns:
            Cached response or None
        """
        cache_data = {"prompt": prompt, "model": model}
        key = self._generate_key(cache_data, "llm")
        return self.get(key)

    def cache_embedding(self, text: str, model: str, embedding: List[float],
                       cost: float = 0.0) -> str:
        """Cache text embedding.

        Args:
            text: Input text
            model: Embedding model name
            embedding: Vector embedding
            cost: Cost of generating embedding

        Returns:
            Cache key
        """
        cache_data = {"text": text, "model": model}
        key = self._generate_key(cache_data, "emb")

        self.set(
            key=key,
            value=embedding,
            cost_saved=cost,
            tags=["embedding", model]
        )

        return key

    def get_embedding(self, text: str, model: str) -> Optional[List[float]]:
        """Get cached embedding.

        Args:
            text: Input text
            model: Embedding model name

        Returns:
            Cached embedding or None
        """
        cache_data = {"text": text, "model": model}
        key = self._generate_key(cache_data, "emb")
        return self.get(key)

    def clean_expired(self) -> int:
        """Clean expired cache entries.

        Returns:
            Number of entries cleaned
        """
        cleaned = 0

        with sqlite3.connect(self.db_path) as conn:
            # Get all entries with TTL
            cursor = conn.execute(
                "SELECT key, created_at, ttl, file_path FROM cache_entries WHERE ttl IS NOT NULL"
            )

            now = time.time()
            for row in cursor:
                key, created_at, ttl, file_path = row
                if now - created_at > ttl:
                    # Remove file
                    file_path = Path(file_path)
                    if file_path.exists():
                        file_path.unlink()

                    # Remove from database
                    conn.execute("DELETE FROM cache_entries WHERE key = ?", (key,))

                    # Remove from memory cache
                    self.memory_cache.pop(key, None)

                    cleaned += 1

        logger.info(f"Cleaned {cleaned} expired cache entries")
        return cleaned

    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics.

        Returns:
            Dictionary with cache stats
        """
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("SELECT COUNT(*), SUM(cost_saved) FROM cache_entries")
            total_entries, total_saved = cursor.fetchone()

            # Calculate cache size
            cache_size = sum(
                f.stat().st_size for f in self.cache_dir.glob("*.pkl")
                if f.is_file()
            )

        hit_rate = self.stats["hits"] / (self.stats["hits"] + self.stats["misses"]) \
                  if (self.stats["hits"] + self.stats["misses"]) > 0 else 0

        return {
            "hit_rate": f"{hit_rate:.2%}",
            "total_entries": total_entries or 0,
            "memory_cache_size": len(self.memory_cache),
            "disk_cache_size_mb": cache_size / (1024 * 1024),
            "total_cost_saved": f"${(total_saved or 0):.2f}",
            "hits": self.stats["hits"],
            "misses": self.stats["misses"]
        }

    def clear_by_tags(self, tags: List[str]) -> int:
        """Clear cache entries by tags.

        Args:
            tags: Tags to match

        Returns:
            Number of entries cleared
        """
        cleared = 0

        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("SELECT key, tags, file_path FROM cache_entries")

            for row in cursor:
                key, entry_tags_json, file_path = row
                entry_tags = json.loads(entry_tags_json) if entry_tags_json else []

                if any(tag in entry_tags for tag in tags):
                    # Remove file
                    file_path = Path(file_path)
                    if file_path.exists():
                        file_path.unlink()

                    # Remove from database
                    conn.execute("DELETE FROM cache_entries WHERE key = ?", (key,))

                    # Remove from memory cache
                    self.memory_cache.pop(key, None)

                    cleared += 1

        logger.info(f"Cleared {cleared} cache entries with tags: {tags}")
        return cleared


# Global cache manager instance
_cache_manager = None


def get_cache_manager() -> CacheManager:
    """Get global cache manager instance."""
    global _cache_manager
    if _cache_manager is None:
        _cache_manager = CacheManager()
    return _cache_manager


def cached_llm_call(model: str, cost_per_call: float = 0.01):
    """Decorator for caching LLM API calls.

    Args:
        model: Model name
        cost_per_call: Estimated cost per API call
    """
    def decorator(func):
        def wrapper(prompt: str, **kwargs):
            cache = get_cache_manager()

            # Try to get from cache first
            cached_response = cache.get_llm_response(prompt, model)
            if cached_response:
                logger.debug(f"Cache hit for LLM call: {model}")
                return cached_response

            # Make actual API call
            response = func(prompt, **kwargs)

            # Cache the response
            cache.cache_llm_response(prompt, model, response, cost_per_call)
            logger.debug(f"Cached LLM response: {model}, cost saved: ${cost_per_call:.4f}")

            return response
        return wrapper
    return decorator
