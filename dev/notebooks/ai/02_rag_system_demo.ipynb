{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System Demo\n",
    "\n",
    "This notebook demonstrates the Retrieval-Augmented Generation (RAG) system including:\n",
    "- Document ingestion and chunking\n",
    "- Vector store integration\n",
    "- Hybrid retrieval methods\n",
    "- Question answering with context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(\"../../../src\")\n",
    "\n",
    "from ai import (\n",
    "    ChunkingStrategy,\n",
    "    Document,\n",
    "    DocumentChunker,\n",
    "    LLMFactory,\n",
    "    RAGConfig,\n",
    "    RAGPipeline,\n",
    "    RetrievalMethod,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sample Documents\n",
    "\n",
    "Let's create some sample documents about AI and machine learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents about AI/ML topics\n",
    "sample_docs = [\n",
    "    Document(\n",
    "        content=\"\"\"\n",
    "        Machine Learning Fundamentals\n",
    "\n",
    "        Machine Learning is a subset of artificial intelligence that enables computers\n",
    "        to learn and make decisions without being explicitly programmed. It uses\n",
    "        algorithms to analyze data, identify patterns, and make predictions.\n",
    "\n",
    "        Key types of machine learning include:\n",
    "        1. Supervised Learning: Uses labeled data to train models\n",
    "        2. Unsupervised Learning: Finds patterns in unlabeled data\n",
    "        3. Reinforcement Learning: Learns through interaction and feedback\n",
    "\n",
    "        Common algorithms include linear regression, decision trees, neural networks,\n",
    "        support vector machines, and ensemble methods like random forests.\n",
    "        \"\"\",\n",
    "        metadata={\n",
    "            \"source\": \"ml_fundamentals.txt\",\n",
    "            \"topic\": \"machine_learning\",\n",
    "            \"difficulty\": \"beginner\",\n",
    "        },\n",
    "    ),\n",
    "    Document(\n",
    "        content=\"\"\"\n",
    "        Deep Learning and Neural Networks\n",
    "\n",
    "        Neural Networks are computing systems inspired by biological neural networks.\n",
    "        They consist of layers of interconnected nodes (neurons) that process information\n",
    "        through weighted connections. Deep learning uses multi-layer neural networks.\n",
    "\n",
    "        Architecture components:\n",
    "        - Input Layer: Receives data\n",
    "        - Hidden Layers: Process information (deep networks have many)\n",
    "        - Output Layer: Produces final results\n",
    "        - Activation Functions: Introduce non-linearity (ReLU, Sigmoid, Tanh)\n",
    "\n",
    "        Popular architectures include Convolutional Neural Networks (CNNs) for images,\n",
    "        Recurrent Neural Networks (RNNs) for sequences, and Transformers for language.\n",
    "\n",
    "        Training uses backpropagation to adjust weights based on error gradients.\n",
    "        \"\"\",\n",
    "        metadata={\n",
    "            \"source\": \"deep_learning.txt\",\n",
    "            \"topic\": \"deep_learning\",\n",
    "            \"difficulty\": \"intermediate\",\n",
    "        },\n",
    "    ),\n",
    "    Document(\n",
    "        content=\"\"\"\n",
    "        Natural Language Processing (NLP)\n",
    "\n",
    "        Natural Language Processing is a branch of AI that helps computers\n",
    "        understand, interpret, and generate human language. It combines computational\n",
    "        linguistics with machine learning and deep learning.\n",
    "\n",
    "        Key NLP tasks include:\n",
    "        - Text Classification: Categorizing text into predefined classes\n",
    "        - Named Entity Recognition: Identifying entities like names, places, dates\n",
    "        - Sentiment Analysis: Determining emotional tone of text\n",
    "        - Machine Translation: Converting text between languages\n",
    "        - Question Answering: Providing answers to natural language questions\n",
    "        - Text Summarization: Creating concise summaries of longer texts\n",
    "\n",
    "        Modern NLP heavily relies on transformer models like BERT, GPT, and T5.\n",
    "        These models use attention mechanisms to understand context and relationships.\n",
    "        \"\"\",\n",
    "        metadata={\n",
    "            \"source\": \"nlp_overview.txt\",\n",
    "            \"topic\": \"nlp\",\n",
    "            \"difficulty\": \"intermediate\",\n",
    "        },\n",
    "    ),\n",
    "    Document(\n",
    "        content=\"\"\"\n",
    "        Large Language Models and Generative AI\n",
    "\n",
    "        Large Language Models (LLMs) are AI systems trained on vast amounts of text data\n",
    "        to understand and generate human-like text. They represent a breakthrough in\n",
    "        natural language understanding and generation.\n",
    "\n",
    "        Key characteristics:\n",
    "        - Scale: Billions or trillions of parameters\n",
    "        - Emergent abilities: Capabilities that arise from scale\n",
    "        - Few-shot learning: Can learn new tasks from few examples\n",
    "        - Generalization: Apply knowledge to new domains\n",
    "\n",
    "        Applications include:\n",
    "        - Chatbots and virtual assistants\n",
    "        - Code generation and programming assistance\n",
    "        - Creative writing and content creation\n",
    "        - Language translation and localization\n",
    "        - Educational tutoring and explanation\n",
    "\n",
    "        Popular models include GPT-4, Claude, PaLM, and LLaMA.\n",
    "        \"\"\",\n",
    "        metadata={\n",
    "            \"source\": \"llm_overview.txt\",\n",
    "            \"topic\": \"llm\",\n",
    "            \"difficulty\": \"advanced\",\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Created {len(sample_docs)} sample documents:\")\n",
    "for doc in sample_docs:\n",
    "    print(\n",
    "        f\"  ‚Ä¢ {doc.metadata['source']}: {doc.metadata['topic']} ({doc.metadata['difficulty']})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Chunking Demo\n",
    "\n",
    "Let's explore different chunking strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different chunking strategies\n",
    "chunker_fixed = DocumentChunker(\n",
    "    chunk_size=300, chunk_overlap=50, strategy=ChunkingStrategy.FIXED_SIZE\n",
    ")\n",
    "chunker_sentence = DocumentChunker(chunk_size=300, strategy=ChunkingStrategy.SENTENCE)\n",
    "\n",
    "# Chunk the first document with different strategies\n",
    "test_doc = sample_docs[0]\n",
    "\n",
    "print(\"Original document length:\", len(test_doc.content))\n",
    "print()\n",
    "\n",
    "# Fixed size chunking\n",
    "fixed_chunks = await chunker_fixed.chunk_document(test_doc)\n",
    "print(f\"Fixed-size chunking: {len(fixed_chunks)} chunks\")\n",
    "for i, chunk in enumerate(fixed_chunks[:2]):  # Show first 2\n",
    "    print(f\"  Chunk {i + 1}: {len(chunk.content)} chars\")\n",
    "    print(f\"    Preview: {chunk.content[:100]}...\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Sentence-based chunking\n",
    "sentence_chunks = await chunker_sentence.chunk_document(test_doc)\n",
    "print(f\"Sentence-based chunking: {len(sentence_chunks)} chunks\")\n",
    "for i, chunk in enumerate(sentence_chunks[:2]):  # Show first 2\n",
    "    print(f\"  Chunk {i + 1}: {len(chunk.content)} chars\")\n",
    "    print(f\"    Preview: {chunk.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Pipeline Setup\n",
    "\n",
    "Now let's create and configure a RAG pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG configuration\n",
    "rag_config = RAGConfig(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    retrieval_method=RetrievalMethod.VECTOR,  # Start with vector retrieval\n",
    "    top_k=3,\n",
    "    similarity_threshold=0.1,\n",
    "    vector_store_type=\"memory\",  # Use in-memory store for demo\n",
    "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    ")\n",
    "\n",
    "print(\"RAG Configuration:\")\n",
    "print(\n",
    "    f\"  Chunking: {rag_config.chunk_size} chars with {rag_config.chunk_overlap} overlap\"\n",
    ")\n",
    "print(f\"  Retrieval: {rag_config.retrieval_method.value}\")\n",
    "print(f\"  Top-K: {rag_config.top_k}\")\n",
    "print(f\"  Vector Store: {rag_config.vector_store_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LLM provider (Note: requires API key in environment)\n",
    "llm_provider = LLMFactory.create(\n",
    "    \"openai\",\n",
    "    {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"temperature\": 0.3,  # Lower temperature for more focused answers\n",
    "        \"max_tokens\": 1000,\n",
    "        \"api_key\": None,  # Should be set via OPENAI_API_KEY environment variable\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"LLM Provider created: OpenAI GPT-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG pipeline\n",
    "rag_pipeline = RAGPipeline(rag_config, llm_provider)\n",
    "\n",
    "print(\"‚úÖ RAG Pipeline created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Ingestion\n",
    "\n",
    "Let's ingest our sample documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest documents\n",
    "print(\"üì• Ingesting documents...\")\n",
    "\n",
    "await rag_pipeline.ingest_documents(sample_docs)\n",
    "\n",
    "print(\"‚úÖ Documents ingested successfully\")\n",
    "print(f\"   Processed {len(sample_docs)} documents\")\n",
    "print(\"   Documents are now chunked and embedded in the vector store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Testing\n",
    "\n",
    "Let's test document retrieval with different queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval with various queries\n",
    "test_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How do neural networks work?\",\n",
    "    \"What are some NLP applications?\",\n",
    "    \"Tell me about large language models\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"üîç Query: {query}\")\n",
    "\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = await rag_pipeline.retrieve(query, k=2)\n",
    "\n",
    "    print(f\"üìö Retrieved {len(retrieved_docs)} documents:\")\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        score = f\" (score: {doc.score:.3f})\" if doc.score else \"\"\n",
    "        source = doc.metadata.get(\"source\", \"unknown\")\n",
    "        topic = doc.metadata.get(\"topic\", \"unknown\")\n",
    "        print(f\"  {i + 1}. {source} ({topic}){score}\")\n",
    "        print(f\"     Preview: {doc.content[:150]}...\")\n",
    "\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering\n",
    "\n",
    "Now let's test the full RAG pipeline with question answering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question answering\n",
    "questions = [\n",
    "    \"What are the three main types of machine learning?\",\n",
    "    \"What are the key components of a neural network architecture?\",\n",
    "    \"What NLP tasks are commonly performed?\",\n",
    "    \"What makes large language models special compared to smaller models?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"‚ùì Question: {question}\")\n",
    "    print()\n",
    "\n",
    "    try:\n",
    "        # Get answer from RAG pipeline\n",
    "        answer = await rag_pipeline.query(question)\n",
    "\n",
    "        print(\"üí° Answer:\")\n",
    "        print(answer)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Retrieval Demo\n",
    "\n",
    "Let's test hybrid retrieval (combining vector and keyword search):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hybrid RAG pipeline\n",
    "hybrid_config = RAGConfig(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    retrieval_method=RetrievalMethod.HYBRID,  # Use hybrid retrieval\n",
    "    top_k=3,\n",
    "    vector_store_type=\"memory\",\n",
    ")\n",
    "\n",
    "hybrid_pipeline = RAGPipeline(hybrid_config, llm_provider)\n",
    "\n",
    "# Ingest documents into hybrid pipeline\n",
    "await hybrid_pipeline.ingest_documents(sample_docs)\n",
    "\n",
    "print(\"‚úÖ Hybrid RAG pipeline created and documents ingested\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare vector vs hybrid retrieval\n",
    "test_query = \"transformers attention mechanisms\"\n",
    "\n",
    "print(f\"üîç Comparing retrieval methods for: '{test_query}'\")\n",
    "print()\n",
    "\n",
    "# Vector retrieval\n",
    "vector_docs = await rag_pipeline.retrieve(test_query)\n",
    "print(\"üìä Vector Retrieval Results:\")\n",
    "for i, doc in enumerate(vector_docs):\n",
    "    source = doc.metadata.get(\"source\", \"unknown\")\n",
    "    score = f\" (score: {doc.score:.3f})\" if doc.score else \"\"\n",
    "    print(f\"  {i + 1}. {source}{score}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Hybrid retrieval\n",
    "hybrid_docs = await hybrid_pipeline.retrieve(test_query)\n",
    "print(\"üîÑ Hybrid Retrieval Results:\")\n",
    "for i, doc in enumerate(hybrid_docs):\n",
    "    source = doc.metadata.get(\"source\", \"unknown\")\n",
    "    score = f\" (score: {doc.score:.3f})\" if doc.score else \"\"\n",
    "    print(f\"  {i + 1}. {source}{score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Evaluation\n",
    "\n",
    "Let's evaluate our RAG system performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation queries with ground truth\n",
    "eval_queries = [\n",
    "    \"What is supervised learning?\",\n",
    "    \"How do transformers work in NLP?\",\n",
    "    \"What are the layers in neural networks?\",\n",
    "]\n",
    "\n",
    "# Ground truth: which documents should be retrieved for each query\n",
    "ground_truth = [\n",
    "    [\"ml_fundamentals.txt\"],  # Supervised learning is in ML fundamentals\n",
    "    [\"nlp_overview.txt\"],  # Transformers mentioned in NLP overview\n",
    "    [\"deep_learning.txt\"],  # Neural network layers in deep learning doc\n",
    "]\n",
    "\n",
    "# Run evaluation\n",
    "try:\n",
    "    eval_results = await rag_pipeline.evaluate_retrieval(eval_queries, ground_truth)\n",
    "\n",
    "    print(\"üìä RAG Evaluation Results:\")\n",
    "    print(f\"  Precision: {eval_results['precision']:.3f}\")\n",
    "    print(f\"  Recall: {eval_results['recall']:.3f}\")\n",
    "    print(f\"  F1 Score: {eval_results['f1']:.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Evaluation failed: {e}\")\n",
    "    print(\"This might be due to document IDs not being set properly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the RAG system capabilities:\n",
    "\n",
    "### ‚úÖ What We Built:\n",
    "1. **Document Processing**: Chunking strategies (fixed-size, sentence-based)\n",
    "2. **Vector Storage**: In-memory vector store with embeddings\n",
    "3. **Retrieval Methods**: Vector similarity and hybrid search\n",
    "4. **Question Answering**: Context-aware answer generation\n",
    "5. **Evaluation**: Performance metrics for retrieval quality\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "- **Production Vector Stores**: ChromaDB, FAISS, Pinecone\n",
    "- **Advanced Chunking**: Semantic chunking, document structure awareness\n",
    "- **Reranking**: Add reranking models for better relevance\n",
    "- **Query Expansion**: Enhance queries for better retrieval\n",
    "- **Evaluation**: More comprehensive evaluation metrics\n",
    "\n",
    "### üîß Configuration Options:\n",
    "- Chunk size and overlap\n",
    "- Retrieval methods (vector/keyword/hybrid)\n",
    "- Similarity thresholds\n",
    "- Top-K retrieval counts\n",
    "- Embedding models"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
