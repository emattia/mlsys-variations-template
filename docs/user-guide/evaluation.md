# Evaluation

> "In God we trust; all others must bring data." — W. Edwards Deming

Topics to be covered:

1. Standard evaluation metrics for tabular / NLP / CV tasks.
2. Generating visualisations (ROC, PR curves, confusion matrices).
3. Reproducible evaluation scripts (`workflows/model_evaluation.py`).
4. Storing outputs under `models/evaluation/` and `reports/`.
5. Benchmarking vs. baselines & ablations.

> **TODO** – add code snippets once evaluation pipeline is finalised.
