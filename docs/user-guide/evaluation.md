# Evaluation

*(This page is a stub and will be expanded in a future update.)*

Topics to be covered:

1. Standard evaluation metrics for tabular / NLP / CV tasks.
2. Generating visualisations (ROC, PR curves, confusion matrices).
3. Reproducible evaluation scripts (`workflows/model_evaluation.py`).
4. Storing outputs under `models/evaluation/` and `reports/`.
5. Benchmarking vs. baselines & ablations.

> **TODO** â€“ add code snippets once evaluation pipeline is finalised.
